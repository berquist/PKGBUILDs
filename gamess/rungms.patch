diff --git a/rungms b/rungms
index e66133e..f811d6d 100755
--- a/rungms
+++ b/rungms
@@ -61,9 +61,9 @@
 #       See also a very old LoadLeveler "ll-gms" for some IBM systems.
 #
 set TARGET=sockets
-set SCR=/scr1/$USER
-set USERSCR=~/gamess-devv
-set GMSPATH=~/gamess-devv
+set SCR="$SCRATCH"
+set USERSCR="$SCRATCH"
+set GMSPATH=/opt/gamess
 #
 set JOB=$1      # name of the input file xxx.inp, give only the xxx part
 set VERNO=$2    # revision number of the executable created by 'lked' step
@@ -421,13 +421,13 @@ if ($TARGET == sockets) then
 #          Apple binary from our web site, provided it is uncommented,
 #          and the passage #2 just above is deleted or commented out.
 #
-#--   set HOSTLIST=()
-#--   @ n=1
-#--   while ($n <= $NCPUS)
-#--      set HOSTLIST=($HOSTLIST localhost)
-#--      @ n++
-#--   end
-#--   set NNODES=$NCPUS
+set HOSTLIST=()
+@ n=1
+while ($n <= $NCPUS)
+   set HOSTLIST=($HOSTLIST localhost)
+   @ n++
+end
+set NNODES=$NCPUS
 #
 #       5. A phony example, of four dual processors (arbitrary names)
 #          Since their names never change, we just can just specify them.
@@ -1303,270 +1303,6 @@ if ($TARGET == ga) then
 endif
 #      ------ end of the GA execution section -------
 
-
-#     SGI Altix or ICE, using ProPack's mpt MPI library, and PBS batch queues
-#
-if ($TARGET == altix) then
-#
-#  James Ianni bumped up two values in the script from Dave Anderson,
-#  but not the first one shown.  Alan Sheinine discovered the final two.
-#
-   set SMP_SIZE=36
-   echo Assuming this Altix has $SMP_SIZE cores/node...
-   set echo
-   setenv MPI_BUFS_THRESHOLD 32
-#    default:  96 pages (1 page = 16KB), Max:  1 million pages
-   setenv MPI_BUFS_PER_PROC 512
-#    default:  32 pages (1 page = 16KB), Max:  1 million pages
-   setenv MPI_BUFS_PER_HOST 32
-#    set number of milliseconds between polls, helps data servers sleep
-   setenv MPI_NAP 1
-#    way to force MPI processes out to every node (each with SMP_SIZE cores)
-   setenv MPI_CONNECTIONS_THRESHOLD $SMP_SIZE
-   unset echo
-
-   setenv GMSPATH /usr/local/u/boatzj/gamess
-   cat ${PBS_NODEFILE} | sort  > $SCR/$JOB.nodes.$$
-   cat $SCR/$JOB.nodes.$$ $SCR/$JOB.nodes.$$ | sort > $SCR/$JOB.2xnodes.$$
-   setenv PBS_NODEFILE $SCR/$JOB.2xnodes.$$
-
-#-debug
-#--   echo "Contents of PBS_NODEFILE are ..."
-#--   cat $PBS_NODEFILE
-#--   echo "PBS_NODEFILE has the following number of hostnames:"
-#--   cat $PBS_NODEFILE | wc -l
-#-debug
-
-   @ NPROCS = $NCPUS + $NCPUS
-   chdir $SCR
-   set echo
-   mpiexec_mpt -v -n $NPROCS $GMSPATH/gamess.$VERNO.x $JOB
-   unset echo
-   rm $SCR/$JOB.nodes.$$
-   rm $SCR/$JOB.2xnodes.$$
-endif
-
-
-#   CRAY-XT (various models) running GAMESS/DDI over MPI wants you to
-#      a) set the path to point to the GAMESS executable
-#      b) set SMP_SIZE to the number of cores in each XT node
-#      c) read the notes below about SCR and USERSCR
-#
-#   This machine runs only one MPI process/core, with most of these
-#   able to be compute processes.  DDI_DS_PER_NODE lets you pick
-#   how many of processes are to function as data servers.
-#   So a node runs SMP_SIZE minus DDI_DE_PER_NODE compute processes.
-#
-#   The TPN variable below lets you use more memory, by wasting
-#   some of the processors, if that is needed to do your run.
-#   The 4th run parameter has to be passed at time of job submission,
-#   if not, all cores are used.
-#
-#   This machine may not allow FORTRAN to access the file server
-#   directly.  As a work-around, input data like the error function
-#   table can to be copied to the working disk SCR.  Any extra
-#   output files can be rescued from USERSCR after the run ends.
-#
-#   For speed reasons, you probably want to set SCR at the top of this
-#   file to /tmp, which is a RAM disk.  Not all data centers will let
-#   you do this, and it is acceptable to use the less eficient
-#   alternative of setting SCR to a /lustre subdirectory.
-#
-#   You should set USERSCR to your directory in /lustre, which is
-#   visible to all compute nodes, but not as fast as its /tmp.
-#   Supplemental output files (like .dat) are then not in a RAM
-#   disk which is wiped automatically at job end.
-#
-#   If you use subgroups, e.g. $GDDI input for FMO runs, you should
-#   modify the input copying near the top of this file to copy to
-#   USERSCR rather than SCR.  A file in /lustre is visible to all
-#   nodes!  You must also change gms-files.csh to define INPUT as
-#   being in USERSCR rather than SCR.
-#
-#   aprun flags:
-#   -n is number of processing elements PEs required for the application
-#   -N is number of MPI tasks per physical node
-#   -d is number of threads per MPI task (interacts w/ OMP_NUM_THREADS)
-#   -r is number of CPU cores to be used for core specialization
-#   -j is number of CPUs to use per compute unit (single stream mode)
-#   If your data center does not let you use -r 1 below, to run on
-#   a jitter-free microkernel, just remove that flag from 'aprun'.
-#
-if ($TARGET == cray-xt) then
-       #   path to binary, and number of cores per node.
-   set GMSPATH=/u/sciteam/spruitt/gamess
-   set SMP_SIZE=16
-
-       # number of processes per node (TPN=tasks/node)
-   set TPN=$4
-   if (null$TPN == null) set TPN=$SMP_SIZE
-   if ($TPN > $SMP_SIZE) set TPN=$SMP_SIZE
-
-   if (!(-e $SCR/$JOB)) mkdir $SCR/$JOB
-
-       # copy auxiliary data files to working disk, redefine their location.
-   cp    $ERICFMT $SCR/$JOB/ericfmt.dat
-   cp -r $MCPPATH $SCR/$JOB/MCP
-   setenv ERICFMT $SCR/$JOB/ericfmt.dat
-   setenv MCPPATH $SCR/$JOB/MCP
-
-       # execute, with a few run-time tunings set first.
-   set echo
-   setenv DDI_DS_PER_NODE 1
-   setenv OMP_NUM_THREADS 1
-#---   setenv MPICH_UNEX_BUFFER_SIZE 90000000
-   setenv MPICH_MAX_SHORT_MSG_SIZE 4000
-   chdir $SCR/$JOB
-   aprun -j 1 -n $NCPUS -N $TPN $GMSPATH/gamess.$VERNO.x $JOB
-   unset echo
-
-#             Rescue the supplementary ASCII output files,
-#             from /lustre to one's permanent disk storage.
-#             This user is doing FMO trajectories, mainly,
-#             and ends up saving all those files...
-   set PERMSCR=/u/sciteam/spruitt/scr
-   if (-e $USERSCR/$JOB.efp)   cp $USERSCR/$JOB.efp   $PERMSCR
-   if (-e $USERSCR/$JOB.gamma) cp $USERSCR/$JOB.gamma $PERMSCR
-   if (-e $USERSCR/$JOB.trj)   cp $USERSCR/$JOB.trj   $PERMSCR
-   if (-e $USERSCR/$JOB.rst)   cp $USERSCR/$JOB.rst   $PERMSCR
-   if (-e $USERSCR/$JOB.dat)   cp $USERSCR/$JOB.dat   $PERMSCR
-                               cp $USERSCR/$JOB.trj.000* $PERMSCR
-   rm -f  $USERSCR/$JOB.*
-#              clean SCR, e.g. the RAM disk /tmp
-   rm -f  $SCR/$JOB/$JOB.F*
-   rm -f  $SCR/$JOB/ericfmt.dat
-   rm -rf $SCR/$JOB/MCP
-   rmdir  $SCR/$JOB
-#             perhaps these next things are batch queue related files?
-#             this is dangerous if jobs are launched from home directory,
-#             as it will wipe out input and output!
-   #---rm -f  /u/sciteam/spruitt/$JOB.*
-endif
-
-
-#   The IBM SP running DDI using mixed LAPI/MPI messaging wants you to
-#      a) set the path to point to the GAMESS executable
-#      b) define hosts in a host file, which are probably defined by
-#         a batch queue system.  An example for LoadLeveler is given.
-#   Please note that most IBM systems schedule their batch jobs with
-#   the LoadLeveler software product.  Please see gamess/tools/llgms for
-#   a "front-end" script that submits this script as a "back-end" job,
-#   with all necessary LL accouterments inserted at the top of the job.
-#
-if ($TARGET == ibm64-sp) then
-#
-#     point this to where your GAMESS executable is located
-   set path=($path /u1/mike/gamess)
-#
-#     error messages defaulted to American English, try C if lacking en_US
-   setenv LOCPATH /usr/lib/nls/loc:/usr/vacpp/bin
-   setenv LANG en_US
-#
-#     this value is picked up inside DDI, then used in a "chdir $SCR"
-   setenv DDI_SCRATCH $SCR
-#
-#     define the name of a host name file.
-#
-   setenv HOSTFILE $SCR/$JOB.poehosts
-   if (-e $HOSTFILE) rm -f $HOSTFILE
-#
-#        If the job was scheduled by LoadLeveler, let LL control everything.
-#
-   if ($?LOADLBATCH) then
-#        just get POE to tell us what nodes we were dynamically assigned to.
-      /usr/bin/poe hostname -stdoutmode ordered > $HOSTFILE
-      set SMP_SIZE = $TPN
-#
-#        Otherwise, if this was not an LoadLeveler job, here's a hack!
-#        It is unlikely this will match your SP's characteristics, as
-#        we just guess its a 4-way node, 4 processors, run interactively.
-#        It is here mainly to illustrate the sort of MP_XXX's you need.
-   else
-      set SMP_SIZE=4
-      set NCPUS=4
-      set NNODES=1
-      echo `hostname`  > $HOSTFILE
-      echo `hostname` >> $HOSTFILE
-      echo `hostname` >> $HOSTFILE
-      echo `hostname` >> $HOSTFILE
-
-      echo "Variables controlling Parallel Environment process kickoff are"
-      set echo
-      setenv MP_NODES          $NNODES
-      setenv MP_PROCS          $NCPUS
-      setenv MP_HOSTFILE       $HOSTFILE
-      setenv MP_CPU_USE        unique
-      setenv MP_ADAPTER_USE    dedicated
-#       GAMESS is implemented using both MPI and LAPI active messages.
-      setenv MP_MSG_API        MPI,LAPI
-      setenv MP_EUILIB         us
-#       SP systems with one switch adapter might use css0, not striping csss
-      setenv MP_EUIDEVICE      csss
-      setenv MP_RESD           no
-      unset echo
-   endif
-
-#       and now we are ready to execute, using poe to kick off the tasks.
-   @ NNODES = ($NCPUS - 1) / $SMP_SIZE + 1
-   echo "Running $NCPUS processes on $NNODES nodes ($SMP_SIZE-way SMP)."
-   set echo
-   /usr/bin/poe gamess.$VERNO.x $JOB -stdinmode none
-   unset echo
-endif
-
-
-#   SGI Origin (a very old machine) running SHMEM wants you to
-#      a) set the path to point to the GAMESS executable
-#   NOTE!  This does not mean SGI Altix or ICE!!!
-#   We've heard that setting the environment variable
-#       SMA_SYMMETRIC_SIZE to 2147483648 (2 GB)
-#   may be helpful if you see DDI_SHPALLOC error messages.
-#
-if ($TARGET == sgi64) then
-   set GMSPATH=/home/hbar4/people/schmidt/gamess
-   chdir $SCR
-   set echo
-   mpirun -np $NCPUS $GMSPATH/gamess.$VERNO.x $JOB < /dev/null
-   unset echo
-endif
-
-
-#   Cray X1 running SHMEM wants you to
-#      a) set the path to point to the GAMESS executable
-#   this is not mentioned above, as it hasn't been verified for ages and ages.
-if ($TARGET == cray-x1) then
-   set GMSPATH=/u1/mike/gamess
-   set OPTS="-m exclusive"
-   if ($NCPUS > 16) then
-      set PERNODE=16
-   else
-      set PERNODE=$NCPUS
-   endif
-   chdir $SCR
-   set echo
-   aprun -c core=0 -n $NCPUS -N $PERNODE $OPTS $GMSPATH/gamess.$VERNO.x $JOB
-   unset echo
-endif
-
-
-#   NEC SX Series wants you to
-#      a) set the path variable to point to the GAMESS executable
-#   this is not mentioned above, as it hasn't been verified for ages and ages.
-if ($TARGET == necsx) then
-   set GMSPATH=/u1/mike/gamess
-
-   chdir $SCR
-   setenv F_RECLUNIT BYTE
-   setenv F_ABORT YES
-   setenv F_ERROPT1 252,252,2,2,1,1,2,1
-   setenv F_PROGINF detail
-   setenv F_SETBUF 4096
-   echo Running $NCPUS compute processes and $NCPUS data server processes...
-   @ NPROCS = $NCPUS + $NCPUS
-   set echo
-   mpirun -np $NPROCS $GMSPATH/gamess.$VERNO.x $JOB < /dev/null
-   unset echo
-endif
 #
 #  ---- the bottom third of the script is to clean up all disk files ----
 #  It is quite useful to display to users how big the disk files got to be.
@@ -1666,25 +1402,6 @@ if ($TARGET == mpi) then
    endif
 endif
 #
-#   IBM SP cleanup code...might need to be something other than 'rsh'.
-#
-if ($TARGET == ibm64-sp) then
-   set lasthost=$master
-   @ n=2   # we already cleaned up the master node just above.
-   @ nmax=$NCPUS
-   while ($n <= $nmax)
-      set host=`sed -n -e "$n p" $HOSTFILE`
-      if ($host != $lasthost) then
-         echo Files used on node $host were:
-         rsh $host "ls -l $SCR/$JOB.*"
-         rsh $host "rm -f $SCR/$JOB.F*"
-         set lasthost=$host
-      endif
-      @ n++
-   end
-   rm -f $HOSTFILE
-endif
-#
 #  and this is the end
 #
 date
